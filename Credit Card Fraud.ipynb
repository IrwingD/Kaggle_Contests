{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection: catching credit card fraud with Python\n",
    "\n",
    "[Card fraud impacts a fraction of one percent of all purchases made with plastic] [src]. Thus even a dumb model that classifies every transaction as \"Not fraudulent\" would give 99% accuracy at worst. This however does not address the business needs of detecting these 1% transactions.\n",
    "\n",
    "The article linked in the first line in this post goes on to explain that in 2015 global Credit card and debit card fraud resulted in losses amounting to $21.84 billion. Card issuers ate 72% of this and merchants 28% of those losses.\n",
    "\n",
    "Kaggle has posted a [great data set] [data_set] that illustrates this point quiet succinctly:\n",
    "\n",
    ">This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It also further goes on to point out that since we have such skewed data calculating model accuracy by itself from the confusion matrix would not carry much meaning.\n",
    "\n",
    "[src]: https://wallethub.com/edu/credit-debit-card-fraud-statistics/25725/\n",
    "[data_set]:https://www.kaggle.com/dalpozz/creditcardfraud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "This dataset has 284,807 rows of credit card transaction data from European cardholders in the month of September 2013 over a period of 2 days. The 28 features of this data set are the principal components obtained after performing a [PCA] [pca_info] on the original data followed by anonymisation to protect [PII] [pii_info], from the description of the data set:\n",
    "\n",
    ">the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "[pca_info]: https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "[pii_info]:https://en.wikipedia.org/wiki/Personally_identifiable_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Success\n",
    "\n",
    "In a Binary Classification Problem it’s often advantageous to use a confusion matrix like the one below in the form of a 2x2 matrix.\n",
    "\n",
    "|                   | Predicted Yes Fraud   | Predicted No Fraud  |\n",
    "|-------------------|-----------------------|---------------------|\n",
    "| Actual Yes Fraud  | true positives        | false negative      |\n",
    "| Actual No Fraud   | false positives       | true negative       |\n",
    "\n",
    "The basic definitions of this are:\n",
    "\n",
    "*  true positives (TP): These are cases in which we predicted yes there is fraud, and in actuality fraud was committed.\n",
    "* true negatives (TN): We predicted no fraud, and in actuality no fraud was committed\n",
    "* false positives (FP): We predicted yes fraud occurred, but there was no fraud perpetrated. (Also known as a \"Type I error.\")\n",
    "* false negatives (FN): We predicted no fraud, but there actually was fraud committed. (Also known as a \"Type II error.\")\n",
    "\n",
    "Using this information a lot of useful parameters can be calculated like:\n",
    "\n",
    "1. Accuracy: Overall, how often is the classifier correct?\n",
    " * = (TP+TN)/total\n",
    "2. Misclassification Rate: Overall, how often is it wrong?\n",
    " * = (FP+FN)/total\n",
    "3. True Positive Rate or \"Recall\" or \"Sensitivity\" : When it's actually fraud, how often does it predict fraud?\n",
    " * = TP/actual fraud = TP/(FN + TP).\n",
    "4. False Positive Rate: When it's actually no fraud , how often does it predict fraud?\n",
    " * = FP/actual no \n",
    "5. Specificity: When it's actually no fraud, how often does it predict no fraud?\n",
    " * = TN/actual no fraud\n",
    " * equivalent to 1 minus False Positive Rate\n",
    "\n",
    "In this case we are most concerned with Recall (without sacrificing too much accuracy) since if it is a fraud we want to catch it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if it really is as badly distributed as claimed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b874e48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEFCAYAAAAmIwo/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqRihokkqdj/AdOJzwloR4osAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b878198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(\"Class\",data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus its clear from the graph that most of the data is not fraudulent (recall that class = 0 is not fraud, and class=1 is fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.ix[:,1:29]\n",
    "Y = data.Class\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Its so common to run a Logistic Regression for a binary target variable with multiple features that I would go as far as to call it a cliché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   89    61]\n",
      " [   10 85283]]\n",
      "Accuracy 99.9169036668 %\n",
      "Recall: 59.33%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(Y_test, Y_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Conclusion\n",
    "Here accuracy is high, but as we discussed earlier that means very little in such a skewed data set, however we do see that recall is quiet poor, we let 40% of the fraudulent transactions through "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is another one of those out of the box \"basic\" algorithms everyone uses, it also happens to be one of my favourites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs =-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  113    37]\n",
      " [    8 85285]]\n",
      "Accuracy 99.9473333099 %\n",
      "Recall: 75.33%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix( Y_test,Y_rf_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Conclusion\n",
    "Here we clearly see that accuracy is much improved but we still let through about 25% of the fraudulent transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "SVM is fairly popular supervised binary classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier= SVC(random_state= 0)\n",
    "classifier.fit(X_train, Y_train.ravel())\n",
    "Y_SVM_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  102    48]\n",
      " [    4 85289]]\n",
      "Accuracy 99.9391407137 %\n",
      "Recall: 68.0%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix( Y_test,Y_SVM_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Conclusion\n",
    "Here we clearly see that accuracy is worse than Random Forest and we let through about 32% of the fraudulent transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayes\n",
    "\n",
    "This algorithm is known to be extremely fast relative to other classification algorithms. It works on Bayes theorem of probability to predict the class of unknown data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NBmodel = GaussianNB()\n",
    "NBmodel.fit(X_train, Y_train)\n",
    "Y_NB_pred= NBmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  124    26]\n",
      " [ 1779 83514]]\n",
      "Accuracy 97.8874805426 %\n",
      "Recall: 82.67%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix( Y_test,Y_NB_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Conclusion\n",
    "Here we clearly see that accuracy is the highest we have got so far but accuracy has fallen a bit, this has happened because we have got a few more False positives, i.e. we tagged some legitimate transacations as fraudulent transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "\n",
    "At this point it is clear that the severly imbalanced data set is causing a few problems to our ML algorithms, a few reasons why this happens is:\n",
    "\n",
    "1. Unequal distribution of the dependent variable causes the performance of existing classifiers to get biased towards the majority class\n",
    "2. ML algorithms (in General) are accuracy driven i.e. they aim to minimize the overall error to which the minority class contributes very little\n",
    "\n",
    "Thus to remedy this situation and improve our models with an unbalanced dataset like this we need to use undersampling. This means training the model on a training set where the “normal” data is undersampled so it has the same size as the fraudulent data.\n",
    "\n",
    "Note that there are otherways to handle imbalanced data but for the purposes of this post Undersampling is what we are going with.[For other options on correcting imbalanced data sets click here] [imba]\n",
    "\n",
    "[imba]: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating undersampled train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud_data = len(data[data.Class == 1])\n",
    "fraud_indices = data[data.Class == 1].index\n",
    "normal_indices = data[data.Class == 0].index\n",
    "undersample_indices = np.random.choice(normal_indices, fraud_data, False)\n",
    "data_undersampled = data.iloc[np.concatenate([fraud_indices,undersample_indices]),:]\n",
    "X_undersampled = data_undersampled.ix[:,1:29]\n",
    "Y_undersampled = data_undersampled.Class\n",
    "X_undersampled_train, X_undersampled_test, Y_undersampled_train, Y_undersampled_test = train_test_split(X_undersampled,Y_undersampled,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on Undersampled training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_us = LogisticRegression()\n",
    "lr_us.fit(X_undersampled_train, Y_undersampled_train)\n",
    "Y_full_lr_pred = lr_us.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of Logistic Regression with undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  138    12]\n",
      " [ 2718 82575]]\n",
      "Accuracy 96.8048874688 %\n",
      "Recall: 92.0%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(Y_test, Y_full_lr_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with undersampled Conclusion\n",
    "Thus we see by just undersampling the training data we have improved our recall tremendously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM on undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_us= SVC(random_state= 0)\n",
    "classifier_us.fit(X_undersampled_train, Y_undersampled_train.ravel())\n",
    "Y_SVM_us_pred = classifier_us.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  141     9]\n",
      " [ 3995 81298]]\n",
      "Accuracy 95.3138349543 %\n",
      "Recall: 94.0%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix( Y_test,Y_SVM_us_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM under sampled data Conclusion\n",
    "Once again we see that we have a minor drop in accuracy but a huge increase in Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayes on Under Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NBmodel_us = GaussianNB()\n",
    "NBmodel_us.fit(X_undersampled_train, Y_undersampled_train)\n",
    "Y_NB_us_pred= NBmodel_us.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  126    24]\n",
      " [ 2797 82496]]\n",
      "Accuracy 96.6983837178 %\n",
      "Recall: 84.0%\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix( Y_test,Y_NB_us_pred,labels=[1,0])\n",
    "print (cnf_matrix)\n",
    "print(\"Accuracy \"+str((cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0] + cnf_matrix[0,1]+cnf_matrix[1,0] + cnf_matrix[1,1])*100) + \" %\")\n",
    "print ('Recall: ' + str(np.round(100*float((cnf_matrix[0][0]))/float((cnf_matrix[0][0]+cnf_matrix[0][1])),2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes undersampled data Conclusion\n",
    "Here undersampling does not seem to have improved the recall much for naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Here we conclude that the algorithms in descending order of recall were:\n",
    "\n",
    "1. SVM on under sampled training data : \n",
    " * Recall: 94.0%\n",
    " * Accuracy 95.31%\n",
    "2. Logistic Regression on under sampled data:\n",
    " * Recall: 92.0%\n",
    " * Accuracy 96.80%\n",
    "3. Naive Bayes on under sampled data:\n",
    " * Recall: 84.0%\n",
    " * Accuracy 96.69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
